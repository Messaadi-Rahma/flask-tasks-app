
==> Audit <==
|------------|----------------------------|----------|--------------|---------|---------------------|---------------------|
|  Command   |            Args            | Profile  |     User     | Version |     Start Time      |      End Time       |
|------------|----------------------------|----------|--------------|---------|---------------------|---------------------|
| start      |                            | minikube | rahma-devops | v1.36.0 | 18 Jun 25 13:25 CET | 18 Jun 25 13:28 CET |
| image      | load rahma/flask-tasks-app | minikube | rahma-devops | v1.36.0 | 18 Jun 25 16:26 CET | 18 Jun 25 16:26 CET |
| image      | load rahma/flask-tasks-app | minikube | rahma-devops | v1.36.0 | 18 Jun 25 16:35 CET | 18 Jun 25 16:35 CET |
| image      | load rahma/flask-tasks-app | minikube | rahma-devops | v1.36.0 | 18 Jun 25 16:46 CET | 18 Jun 25 16:46 CET |
| docker-env |                            | minikube | rahma-devops | v1.36.0 | 18 Jun 25 16:47 CET |                     |
| docker-env |                            | minikube | rahma-devops | v1.36.0 | 18 Jun 25 16:47 CET |                     |
| start      | --driver=docker            | minikube | rahma-devops | v1.36.0 | 18 Jun 25 16:48 CET |                     |
| start      | --driver=docker            | minikube | rahma-devops | v1.36.0 | 18 Jun 25 16:50 CET |                     |
| start      | --driver=docker            | minikube | rahma-devops | v1.36.0 | 20 Jun 25 22:11 CET | 20 Jun 25 22:11 CET |
| docker-env |                            | minikube | rahma-devops | v1.36.0 | 20 Jun 25 22:12 CET | 20 Jun 25 22:12 CET |
| image      | load rahma/flask-tasks-app | minikube | rahma-devops | v1.36.0 | 20 Jun 25 22:12 CET | 20 Jun 25 22:12 CET |
| service    | flask-tasks-service        | minikube | rahma-devops | v1.36.0 | 20 Jun 25 22:26 CET |                     |
|------------|----------------------------|----------|--------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/06/20 22:11:13
Running on machine: Rahma-DevOps
Binary: Built with gc go1.24.0 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0620 22:11:13.392481   10125 out.go:345] Setting OutFile to fd 1 ...
I0620 22:11:13.392638   10125 out.go:397] isatty.IsTerminal(1) = true
I0620 22:11:13.392645   10125 out.go:358] Setting ErrFile to fd 2...
I0620 22:11:13.392654   10125 out.go:397] isatty.IsTerminal(2) = true
I0620 22:11:13.392957   10125 root.go:338] Updating PATH: /home/rahma-devops/.minikube/bin
W0620 22:11:13.393103   10125 root.go:314] Error reading config file at /home/rahma-devops/.minikube/config/config.json: open /home/rahma-devops/.minikube/config/config.json: no such file or directory
I0620 22:11:13.393841   10125 out.go:352] Setting JSON to false
I0620 22:11:13.396567   10125 start.go:130] hostinfo: {"hostname":"Rahma-DevOps","uptime":1579,"bootTime":1750452294,"procs":335,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.11.0-26-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"8c0445b1-430c-4e46-848a-b59af259d67e"}
I0620 22:11:13.396745   10125 start.go:140] virtualization: kvm host
I0620 22:11:13.397988   10125 out.go:177] üòÑ  minikube v1.36.0 on Ubuntu 24.04
I0620 22:11:13.399731   10125 notify.go:220] Checking for updates...
I0620 22:11:13.400173   10125 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0620 22:11:13.401567   10125 driver.go:404] Setting default libvirt URI to qemu:///system
I0620 22:11:13.438040   10125 docker.go:123] docker version: linux-27.5.1:
I0620 22:11:13.438165   10125 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0620 22:11:13.469912   10125 info.go:266] docker info: {ID:3b8009f1-08ba-4847-aa9d-7a1024cae411 Containers:15 ContainersRunning:0 ContainersPaused:0 ContainersStopped:15 Images:26 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:41 SystemTime:2025-06-20 22:11:13.456904964 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.11.0-26-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16438452224 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Rahma-DevOps Labels:[] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0620 22:11:13.470029   10125 docker.go:318] overlay module found
I0620 22:11:13.471514   10125 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0620 22:11:13.472073   10125 start.go:304] selected driver: docker
I0620 22:11:13.472082   10125 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/rahma-devops:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0620 22:11:13.472274   10125 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0620 22:11:13.472482   10125 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0620 22:11:13.509865   10125 info.go:266] docker info: {ID:3b8009f1-08ba-4847-aa9d-7a1024cae411 Containers:15 ContainersRunning:0 ContainersPaused:0 ContainersStopped:15 Images:26 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:41 SystemTime:2025-06-20 22:11:13.497320565 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.11.0-26-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16438452224 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Rahma-DevOps Labels:[] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0620 22:11:13.511219   10125 cni.go:84] Creating CNI manager for ""
I0620 22:11:13.511293   10125 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0620 22:11:13.511350   10125 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/rahma-devops:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0620 22:11:13.512119   10125 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0620 22:11:13.512672   10125 cache.go:121] Beginning downloading kic base image for docker with docker
I0620 22:11:13.513155   10125 out.go:177] üöú  Pulling base image v0.0.47 ...
I0620 22:11:13.513849   10125 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0620 22:11:13.513938   10125 preload.go:146] Found local preload: /home/rahma-devops/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0620 22:11:13.513947   10125 cache.go:56] Caching tarball of preloaded images
I0620 22:11:13.513989   10125 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0620 22:11:13.514143   10125 preload.go:172] Found /home/rahma-devops/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0620 22:11:13.514161   10125 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0620 22:11:13.514361   10125 profile.go:143] Saving config to /home/rahma-devops/.minikube/profiles/minikube/config.json ...
I0620 22:11:13.554510   10125 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon, skipping pull
I0620 22:11:13.554528   10125 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in daemon, skipping load
I0620 22:11:13.554549   10125 cache.go:230] Successfully downloaded all kic artifacts
I0620 22:11:13.554585   10125 start.go:360] acquireMachinesLock for minikube: {Name:mkd446b15a1afa1ec756b137dd05aed66fcb2f7c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0620 22:11:13.554773   10125 start.go:364] duration metric: took 152.516¬µs to acquireMachinesLock for "minikube"
I0620 22:11:13.554799   10125 start.go:96] Skipping create...Using existing machine configuration
I0620 22:11:13.554805   10125 fix.go:54] fixHost starting: 
I0620 22:11:13.555202   10125 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0620 22:11:13.578361   10125 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0620 22:11:13.578396   10125 fix.go:138] unexpected machine state, will restart: <nil>
I0620 22:11:13.579204   10125 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0620 22:11:13.579803   10125 cli_runner.go:164] Run: docker start minikube
I0620 22:11:14.011210   10125 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0620 22:11:14.037303   10125 kic.go:430] container "minikube" state is running.
I0620 22:11:14.037937   10125 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0620 22:11:14.064496   10125 profile.go:143] Saving config to /home/rahma-devops/.minikube/profiles/minikube/config.json ...
I0620 22:11:14.064888   10125 machine.go:93] provisionDockerMachine start ...
I0620 22:11:14.064981   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:14.091149   10125 main.go:141] libmachine: Using SSH client type: native
I0620 22:11:14.091730   10125 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0620 22:11:14.091742   10125 main.go:141] libmachine: About to run SSH command:
hostname
I0620 22:11:14.092597   10125 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:44090->127.0.0.1:32768: read: connection reset by peer
I0620 22:11:17.253451   10125 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0620 22:11:17.253481   10125 ubuntu.go:169] provisioning hostname "minikube"
I0620 22:11:17.253599   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:17.280365   10125 main.go:141] libmachine: Using SSH client type: native
I0620 22:11:17.280767   10125 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0620 22:11:17.280778   10125 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0620 22:11:17.467651   10125 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0620 22:11:17.467761   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:17.492200   10125 main.go:141] libmachine: Using SSH client type: native
I0620 22:11:17.492569   10125 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0620 22:11:17.492588   10125 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0620 22:11:17.649353   10125 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0620 22:11:17.649428   10125 ubuntu.go:175] set auth options {CertDir:/home/rahma-devops/.minikube CaCertPath:/home/rahma-devops/.minikube/certs/ca.pem CaPrivateKeyPath:/home/rahma-devops/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/rahma-devops/.minikube/machines/server.pem ServerKeyPath:/home/rahma-devops/.minikube/machines/server-key.pem ClientKeyPath:/home/rahma-devops/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/rahma-devops/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/rahma-devops/.minikube}
I0620 22:11:17.649479   10125 ubuntu.go:177] setting up certificates
I0620 22:11:17.649501   10125 provision.go:84] configureAuth start
I0620 22:11:17.649642   10125 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0620 22:11:17.698948   10125 provision.go:143] copyHostCerts
I0620 22:11:17.701396   10125 exec_runner.go:144] found /home/rahma-devops/.minikube/ca.pem, removing ...
I0620 22:11:17.701689   10125 exec_runner.go:203] rm: /home/rahma-devops/.minikube/ca.pem
I0620 22:11:17.701831   10125 exec_runner.go:151] cp: /home/rahma-devops/.minikube/certs/ca.pem --> /home/rahma-devops/.minikube/ca.pem (1094 bytes)
I0620 22:11:17.702294   10125 exec_runner.go:144] found /home/rahma-devops/.minikube/cert.pem, removing ...
I0620 22:11:17.702305   10125 exec_runner.go:203] rm: /home/rahma-devops/.minikube/cert.pem
I0620 22:11:17.702373   10125 exec_runner.go:151] cp: /home/rahma-devops/.minikube/certs/cert.pem --> /home/rahma-devops/.minikube/cert.pem (1135 bytes)
I0620 22:11:17.702761   10125 exec_runner.go:144] found /home/rahma-devops/.minikube/key.pem, removing ...
I0620 22:11:17.702772   10125 exec_runner.go:203] rm: /home/rahma-devops/.minikube/key.pem
I0620 22:11:17.702832   10125 exec_runner.go:151] cp: /home/rahma-devops/.minikube/certs/key.pem --> /home/rahma-devops/.minikube/key.pem (1679 bytes)
I0620 22:11:17.703100   10125 provision.go:117] generating server cert: /home/rahma-devops/.minikube/machines/server.pem ca-key=/home/rahma-devops/.minikube/certs/ca.pem private-key=/home/rahma-devops/.minikube/certs/ca-key.pem org=rahma-devops.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0620 22:11:17.866645   10125 provision.go:177] copyRemoteCerts
I0620 22:11:17.866697   10125 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0620 22:11:17.866729   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:17.890078   10125 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rahma-devops/.minikube/machines/minikube/id_rsa Username:docker}
I0620 22:11:18.018962   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1094 bytes)
I0620 22:11:18.069622   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0620 22:11:18.105428   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0620 22:11:18.159135   10125 provision.go:87] duration metric: took 509.615204ms to configureAuth
I0620 22:11:18.159162   10125 ubuntu.go:193] setting minikube options for container-runtime
I0620 22:11:18.159461   10125 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0620 22:11:18.159594   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:18.188209   10125 main.go:141] libmachine: Using SSH client type: native
I0620 22:11:18.188708   10125 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0620 22:11:18.188729   10125 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0620 22:11:18.351832   10125 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0620 22:11:18.351854   10125 ubuntu.go:71] root file system type: overlay
I0620 22:11:18.352033   10125 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0620 22:11:18.352134   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:18.376949   10125 main.go:141] libmachine: Using SSH client type: native
I0620 22:11:18.377350   10125 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0620 22:11:18.377452   10125 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0620 22:11:18.549015   10125 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0620 22:11:18.549114   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:18.571020   10125 main.go:141] libmachine: Using SSH client type: native
I0620 22:11:18.571413   10125 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x83bb20] 0x83e820 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0620 22:11:18.571434   10125 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0620 22:11:18.733060   10125 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0620 22:11:18.733082   10125 machine.go:96] duration metric: took 4.66818213s to provisionDockerMachine
I0620 22:11:18.733097   10125 start.go:293] postStartSetup for "minikube" (driver="docker")
I0620 22:11:18.733113   10125 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0620 22:11:18.733209   10125 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0620 22:11:18.733296   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:18.757592   10125 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rahma-devops/.minikube/machines/minikube/id_rsa Username:docker}
I0620 22:11:18.870827   10125 ssh_runner.go:195] Run: cat /etc/os-release
I0620 22:11:18.876250   10125 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0620 22:11:18.876277   10125 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0620 22:11:18.876287   10125 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0620 22:11:18.876293   10125 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0620 22:11:18.876313   10125 filesync.go:126] Scanning /home/rahma-devops/.minikube/addons for local assets ...
I0620 22:11:18.876652   10125 filesync.go:126] Scanning /home/rahma-devops/.minikube/files for local assets ...
I0620 22:11:18.876775   10125 start.go:296] duration metric: took 143.669285ms for postStartSetup
I0620 22:11:18.876852   10125 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0620 22:11:18.876902   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:18.899632   10125 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rahma-devops/.minikube/machines/minikube/id_rsa Username:docker}
I0620 22:11:19.006535   10125 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0620 22:11:19.013594   10125 fix.go:56] duration metric: took 5.458780862s for fixHost
I0620 22:11:19.013612   10125 start.go:83] releasing machines lock for "minikube", held for 5.458827457s
I0620 22:11:19.013722   10125 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0620 22:11:19.036160   10125 ssh_runner.go:195] Run: cat /version.json
I0620 22:11:19.036258   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:19.036350   10125 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0620 22:11:19.036433   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:19.061551   10125 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rahma-devops/.minikube/machines/minikube/id_rsa Username:docker}
I0620 22:11:19.063549   10125 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rahma-devops/.minikube/machines/minikube/id_rsa Username:docker}
I0620 22:11:19.450699   10125 ssh_runner.go:195] Run: systemctl --version
I0620 22:11:19.459803   10125 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0620 22:11:19.467783   10125 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0620 22:11:19.497930   10125 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0620 22:11:19.498011   10125 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0620 22:11:19.509822   10125 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0620 22:11:19.509841   10125 start.go:495] detecting cgroup driver to use...
I0620 22:11:19.509878   10125 detect.go:190] detected "systemd" cgroup driver on host os
I0620 22:11:19.510014   10125 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0620 22:11:19.532315   10125 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0620 22:11:19.547883   10125 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0620 22:11:19.561957   10125 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0620 22:11:19.562017   10125 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0620 22:11:19.575749   10125 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0620 22:11:19.588448   10125 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0620 22:11:19.602663   10125 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0620 22:11:19.616428   10125 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0620 22:11:19.629458   10125 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0620 22:11:19.644736   10125 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0620 22:11:19.659916   10125 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0620 22:11:19.674887   10125 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0620 22:11:19.686767   10125 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0620 22:11:19.686838   10125 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I0620 22:11:19.702749   10125 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0620 22:11:19.715866   10125 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0620 22:11:19.797625   10125 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0620 22:11:19.911523   10125 start.go:495] detecting cgroup driver to use...
I0620 22:11:19.911577   10125 detect.go:190] detected "systemd" cgroup driver on host os
I0620 22:11:19.911771   10125 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0620 22:11:19.930372   10125 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0620 22:11:19.930450   10125 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0620 22:11:19.950115   10125 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0620 22:11:19.978985   10125 ssh_runner.go:195] Run: which cri-dockerd
I0620 22:11:19.985590   10125 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0620 22:11:20.001464   10125 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0620 22:11:20.028972   10125 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0620 22:11:20.113479   10125 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0620 22:11:20.185584   10125 docker.go:587] configuring docker to use "systemd" as cgroup driver...
I0620 22:11:20.185693   10125 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0620 22:11:20.211012   10125 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0620 22:11:20.224835   10125 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0620 22:11:20.298677   10125 ssh_runner.go:195] Run: sudo systemctl restart docker
I0620 22:11:22.442056   10125 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.143350257s)
I0620 22:11:22.442143   10125 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0620 22:11:22.458193   10125 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0620 22:11:22.473903   10125 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0620 22:11:22.488813   10125 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0620 22:11:22.560814   10125 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0620 22:11:22.631238   10125 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0620 22:11:22.700016   10125 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0620 22:11:22.741007   10125 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0620 22:11:22.758874   10125 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0620 22:11:22.833646   10125 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0620 22:11:23.083080   10125 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0620 22:11:23.103563   10125 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0620 22:11:23.103677   10125 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0620 22:11:23.109684   10125 start.go:563] Will wait 60s for crictl version
I0620 22:11:23.109766   10125 ssh_runner.go:195] Run: which crictl
I0620 22:11:23.115416   10125 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0620 22:11:23.224369   10125 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0620 22:11:23.224461   10125 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0620 22:11:23.314803   10125 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0620 22:11:23.350369   10125 out.go:235] üê≥  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0620 22:11:23.350834   10125 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0620 22:11:23.381155   10125 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0620 22:11:23.389741   10125 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0620 22:11:23.409566   10125 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/rahma-devops:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0620 22:11:23.409696   10125 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0620 22:11:23.409752   10125 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0620 22:11:23.437167   10125 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0620 22:11:23.437206   10125 docker.go:632] Images already preloaded, skipping extraction
I0620 22:11:23.437291   10125 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0620 22:11:23.466292   10125 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0620 22:11:23.466316   10125 cache_images.go:84] Images are preloaded, skipping loading
I0620 22:11:23.466342   10125 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0620 22:11:23.466558   10125 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0620 22:11:23.466652   10125 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0620 22:11:23.643817   10125 cni.go:84] Creating CNI manager for ""
I0620 22:11:23.643844   10125 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0620 22:11:23.643857   10125 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0620 22:11:23.643886   10125 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0620 22:11:23.644058   10125 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0620 22:11:23.644161   10125 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0620 22:11:23.661255   10125 binaries.go:44] Found k8s binaries, skipping transfer
I0620 22:11:23.661340   10125 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0620 22:11:23.679729   10125 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0620 22:11:23.707569   10125 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0620 22:11:23.732959   10125 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0620 22:11:23.761127   10125 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0620 22:11:23.768660   10125 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0620 22:11:23.788082   10125 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0620 22:11:23.873105   10125 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0620 22:11:23.900987   10125 certs.go:68] Setting up /home/rahma-devops/.minikube/profiles/minikube for IP: 192.168.49.2
I0620 22:11:23.901040   10125 certs.go:194] generating shared ca certs ...
I0620 22:11:23.901090   10125 certs.go:226] acquiring lock for ca certs: {Name:mkc23755164bf6ab7a56788e8d43660e41bc7e27 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0620 22:11:23.902805   10125 certs.go:235] skipping valid "minikubeCA" ca cert: /home/rahma-devops/.minikube/ca.key
I0620 22:11:23.903096   10125 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/rahma-devops/.minikube/proxy-client-ca.key
I0620 22:11:23.903116   10125 certs.go:256] generating profile certs ...
I0620 22:11:23.903407   10125 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/rahma-devops/.minikube/profiles/minikube/client.key
I0620 22:11:23.903625   10125 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/rahma-devops/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0620 22:11:23.903825   10125 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/rahma-devops/.minikube/profiles/minikube/proxy-client.key
I0620 22:11:23.904022   10125 certs.go:484] found cert: /home/rahma-devops/.minikube/certs/ca-key.pem (1679 bytes)
I0620 22:11:23.904068   10125 certs.go:484] found cert: /home/rahma-devops/.minikube/certs/ca.pem (1094 bytes)
I0620 22:11:23.904106   10125 certs.go:484] found cert: /home/rahma-devops/.minikube/certs/cert.pem (1135 bytes)
I0620 22:11:23.904140   10125 certs.go:484] found cert: /home/rahma-devops/.minikube/certs/key.pem (1679 bytes)
I0620 22:11:23.905726   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0620 22:11:23.942497   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0620 22:11:23.982047   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0620 22:11:24.019906   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0620 22:11:24.059023   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0620 22:11:24.109049   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0620 22:11:24.161249   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0620 22:11:24.210201   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0620 22:11:24.252295   10125 ssh_runner.go:362] scp /home/rahma-devops/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0620 22:11:24.295333   10125 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0620 22:11:24.325194   10125 ssh_runner.go:195] Run: openssl version
I0620 22:11:24.335030   10125 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0620 22:11:24.348963   10125 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0620 22:11:24.354287   10125 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 18 12:27 /usr/share/ca-certificates/minikubeCA.pem
I0620 22:11:24.354336   10125 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0620 22:11:24.363297   10125 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0620 22:11:24.377317   10125 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0620 22:11:24.382454   10125 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0620 22:11:24.392849   10125 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0620 22:11:24.402106   10125 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0620 22:11:24.411096   10125 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0620 22:11:24.420295   10125 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0620 22:11:24.429613   10125 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0620 22:11:24.438434   10125 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/rahma-devops:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0620 22:11:24.438554   10125 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0620 22:11:24.460971   10125 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0620 22:11:24.475292   10125 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0620 22:11:24.475303   10125 kubeadm.go:589] restartPrimaryControlPlane start ...
I0620 22:11:24.475367   10125 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0620 22:11:24.487847   10125 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0620 22:11:24.489166   10125 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I0620 22:11:24.498268   10125 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0620 22:11:24.513963   10125 kubeadm.go:626] The running cluster does not require reconfiguration: 192.168.49.2
I0620 22:11:24.513991   10125 kubeadm.go:593] duration metric: took 38.682483ms to restartPrimaryControlPlane
I0620 22:11:24.514001   10125 kubeadm.go:394] duration metric: took 75.580234ms to StartCluster
I0620 22:11:24.514019   10125 settings.go:142] acquiring lock: {Name:mkf45f6327a01a19ac085e593a9ffac45c7026d9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0620 22:11:24.514158   10125 settings.go:150] Updating kubeconfig:  /home/rahma-devops/.kube/config
I0620 22:11:24.514925   10125 lock.go:35] WriteFile acquiring /home/rahma-devops/.kube/config: {Name:mk714ea4ae1343f3ba13b5d5536ea556225405f0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0620 22:11:24.515331   10125 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0620 22:11:24.515418   10125 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0620 22:11:24.515510   10125 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0620 22:11:24.515526   10125 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0620 22:11:24.515532   10125 addons.go:247] addon storage-provisioner should already be in state true
I0620 22:11:24.515560   10125 host.go:66] Checking if "minikube" exists ...
I0620 22:11:24.515562   10125 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0620 22:11:24.515565   10125 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0620 22:11:24.515593   10125 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0620 22:11:24.515954   10125 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0620 22:11:24.515990   10125 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0620 22:11:24.516664   10125 out.go:177] üîé  Verifying Kubernetes components...
I0620 22:11:24.517406   10125 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0620 22:11:24.544345   10125 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0620 22:11:24.544366   10125 addons.go:247] addon default-storageclass should already be in state true
I0620 22:11:24.544407   10125 host.go:66] Checking if "minikube" exists ...
I0620 22:11:24.545324   10125 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0620 22:11:24.546210   10125 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0620 22:11:24.547178   10125 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0620 22:11:24.547210   10125 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0620 22:11:24.547333   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:24.584920   10125 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0620 22:11:24.584948   10125 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0620 22:11:24.585068   10125 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0620 22:11:24.586322   10125 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rahma-devops/.minikube/machines/minikube/id_rsa Username:docker}
I0620 22:11:24.619212   10125 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/rahma-devops/.minikube/machines/minikube/id_rsa Username:docker}
I0620 22:11:24.660656   10125 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0620 22:11:24.693430   10125 api_server.go:52] waiting for apiserver process to appear ...
I0620 22:11:24.693550   10125 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0620 22:11:24.727536   10125 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0620 22:11:24.759201   10125 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0620 22:11:24.903870   10125 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0620 22:11:24.903943   10125 retry.go:31] will retry after 179.218906ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0620 22:11:24.905605   10125 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0620 22:11:24.905629   10125 retry.go:31] will retry after 298.627997ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0620 22:11:25.084039   10125 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0620 22:11:25.193640   10125 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0620 22:11:25.194859   10125 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0620 22:11:25.194893   10125 retry.go:31] will retry after 332.431534ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0620 22:11:25.205556   10125 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0620 22:11:25.224058   10125 api_server.go:72] duration metric: took 708.680758ms to wait for apiserver process to appear ...
I0620 22:11:25.224093   10125 api_server.go:88] waiting for apiserver healthz status ...
I0620 22:11:25.224130   10125 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0620 22:11:25.224876   10125 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0620 22:11:25.528310   10125 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0620 22:11:25.724935   10125 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0620 22:11:26.976496   10125 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0620 22:11:26.976516   10125 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0620 22:11:26.976530   10125 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0620 22:11:26.983510   10125 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W0620 22:11:26.983531   10125 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I0620 22:11:27.045273   10125 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.839682265s)
I0620 22:11:27.224953   10125 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0620 22:11:27.230884   10125 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0620 22:11:27.230904   10125 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0620 22:11:27.482248   10125 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.95390993s)
I0620 22:11:27.482968   10125 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I0620 22:11:27.483416   10125 addons.go:514] duration metric: took 2.968010791s for enable addons: enabled=[default-storageclass storage-provisioner]
I0620 22:11:27.724380   10125 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0620 22:11:27.734046   10125 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0620 22:11:27.734098   10125 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0620 22:11:28.224353   10125 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0620 22:11:28.230708   10125 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0620 22:11:28.230732   10125 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0620 22:11:28.724847   10125 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0620 22:11:28.730983   10125 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0620 22:11:28.732788   10125 api_server.go:141] control plane version: v1.33.1
I0620 22:11:28.732820   10125 api_server.go:131] duration metric: took 3.508716118s to wait for apiserver health ...
I0620 22:11:28.732831   10125 system_pods.go:43] waiting for kube-system pods to appear ...
I0620 22:11:28.741653   10125 system_pods.go:59] 7 kube-system pods found
I0620 22:11:28.741693   10125 system_pods.go:61] "coredns-674b8bbfcf-ftzhx" [4ef9ef6d-cafc-4c08-b9f4-a75b2ee540ed] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0620 22:11:28.741706   10125 system_pods.go:61] "etcd-minikube" [3501cd0c-6565-4d4e-96fe-4039e30bb6e2] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0620 22:11:28.741727   10125 system_pods.go:61] "kube-apiserver-minikube" [5c24daec-ebc7-4b7f-aac0-d99c88c116f7] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0620 22:11:28.741738   10125 system_pods.go:61] "kube-controller-manager-minikube" [c5bf05eb-b03a-4798-9742-ed5bcadf8288] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0620 22:11:28.741747   10125 system_pods.go:61] "kube-proxy-h5rgz" [fdf9af22-0128-45cc-9c82-cdad9b998c07] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0620 22:11:28.741763   10125 system_pods.go:61] "kube-scheduler-minikube" [73c5bff1-360e-4a98-bd88-b12831e5cc7b] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0620 22:11:28.741774   10125 system_pods.go:61] "storage-provisioner" [f12fbad4-973f-4ede-aa7c-28c33c98c0d3] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0620 22:11:28.741784   10125 system_pods.go:74] duration metric: took 8.945338ms to wait for pod list to return data ...
I0620 22:11:28.741802   10125 kubeadm.go:578] duration metric: took 4.22643965s to wait for: map[apiserver:true system_pods:true]
I0620 22:11:28.741826   10125 node_conditions.go:102] verifying NodePressure condition ...
I0620 22:11:28.746801   10125 node_conditions.go:122] node storage ephemeral capacity is 153703184Ki
I0620 22:11:28.746845   10125 node_conditions.go:123] node cpu capacity is 12
I0620 22:11:28.746882   10125 node_conditions.go:105] duration metric: took 5.04854ms to run NodePressure ...
I0620 22:11:28.746904   10125 start.go:241] waiting for startup goroutines ...
I0620 22:11:28.746917   10125 start.go:246] waiting for cluster config update ...
I0620 22:11:28.746935   10125 start.go:255] writing updated cluster config ...
I0620 22:11:28.747453   10125 ssh_runner.go:195] Run: rm -f paused
I0620 22:11:28.909557   10125 start.go:607] kubectl: 1.33.1, cluster: 1.33.1 (minor skew: 0)
I0620 22:11:28.910561   10125 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 20 21:11:19 minikube dockerd[738]: time="2025-06-20T21:11:19.989934226Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jun 20 21:11:19 minikube dockerd[738]: time="2025-06-20T21:11:19.996345838Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jun 20 21:11:20 minikube dockerd[738]: time="2025-06-20T21:11:20.024086292Z" level=info msg="Loading containers: start."
Jun 20 21:11:20 minikube dockerd[738]: time="2025-06-20T21:11:20.312409355Z" level=info msg="Processing signal 'terminated'"
Jun 20 21:11:21 minikube dockerd[738]: time="2025-06-20T21:11:21.023947758Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count baeec767a0b13cac63a984ea3babc56b95c82a68eb483a11627f5c60e0c4aed2], retrying...."
Jun 20 21:11:21 minikube dockerd[738]: time="2025-06-20T21:11:21.084702676Z" level=info msg="Loading containers: done."
Jun 20 21:11:21 minikube dockerd[738]: time="2025-06-20T21:11:21.095301528Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jun 20 21:11:21 minikube dockerd[738]: time="2025-06-20T21:11:21.095382914Z" level=info msg="Initializing buildkit"
Jun 20 21:11:21 minikube dockerd[738]: time="2025-06-20T21:11:21.126963007Z" level=info msg="Completed buildkit initialization"
Jun 20 21:11:21 minikube dockerd[738]: time="2025-06-20T21:11:21.133944043Z" level=info msg="Daemon has completed initialization"
Jun 20 21:11:21 minikube dockerd[738]: time="2025-06-20T21:11:21.134028422Z" level=info msg="API listen on /var/run/docker.sock"
Jun 20 21:11:21 minikube dockerd[738]: time="2025-06-20T21:11:21.134035432Z" level=info msg="API listen on [::]:2376"
Jun 20 21:11:21 minikube dockerd[738]: time="2025-06-20T21:11:21.135211538Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jun 20 21:11:21 minikube dockerd[738]: time="2025-06-20T21:11:21.135489609Z" level=info msg="Daemon shutdown complete"
Jun 20 21:11:21 minikube systemd[1]: docker.service: Deactivated successfully.
Jun 20 21:11:21 minikube systemd[1]: Stopped Docker Application Container Engine.
Jun 20 21:11:21 minikube systemd[1]: Starting Docker Application Container Engine...
Jun 20 21:11:21 minikube dockerd[1050]: time="2025-06-20T21:11:21.192019190Z" level=info msg="Starting up"
Jun 20 21:11:21 minikube dockerd[1050]: time="2025-06-20T21:11:21.192915419Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jun 20 21:11:21 minikube dockerd[1050]: time="2025-06-20T21:11:21.206037666Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jun 20 21:11:21 minikube dockerd[1050]: time="2025-06-20T21:11:21.210348611Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jun 20 21:11:21 minikube dockerd[1050]: time="2025-06-20T21:11:21.220077450Z" level=info msg="Loading containers: start."
Jun 20 21:11:22 minikube dockerd[1050]: time="2025-06-20T21:11:22.332665235Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 5d1663f87cd503dec0548c2aed5e1e3d2ff6ff19ba544fb565630a9d0d108a01], retrying...."
Jun 20 21:11:22 minikube dockerd[1050]: time="2025-06-20T21:11:22.394057462Z" level=info msg="Loading containers: done."
Jun 20 21:11:22 minikube dockerd[1050]: time="2025-06-20T21:11:22.404748958Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jun 20 21:11:22 minikube dockerd[1050]: time="2025-06-20T21:11:22.404839838Z" level=info msg="Initializing buildkit"
Jun 20 21:11:22 minikube dockerd[1050]: time="2025-06-20T21:11:22.434784074Z" level=info msg="Completed buildkit initialization"
Jun 20 21:11:22 minikube dockerd[1050]: time="2025-06-20T21:11:22.439552654Z" level=info msg="Daemon has completed initialization"
Jun 20 21:11:22 minikube dockerd[1050]: time="2025-06-20T21:11:22.439638039Z" level=info msg="API listen on /var/run/docker.sock"
Jun 20 21:11:22 minikube dockerd[1050]: time="2025-06-20T21:11:22.439676208Z" level=info msg="API listen on [::]:2376"
Jun 20 21:11:22 minikube systemd[1]: Started Docker Application Container Engine.
Jun 20 21:11:22 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jun 20 21:11:23 minikube cri-dockerd[1368]: time="2025-06-20T21:11:23Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jun 20 21:11:23 minikube cri-dockerd[1368]: time="2025-06-20T21:11:23Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jun 20 21:11:23 minikube cri-dockerd[1368]: time="2025-06-20T21:11:23Z" level=info msg="Start docker client with request timeout 0s"
Jun 20 21:11:23 minikube cri-dockerd[1368]: time="2025-06-20T21:11:23Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jun 20 21:11:23 minikube cri-dockerd[1368]: time="2025-06-20T21:11:23Z" level=info msg="Loaded network plugin cni"
Jun 20 21:11:23 minikube cri-dockerd[1368]: time="2025-06-20T21:11:23Z" level=info msg="Docker cri networking managed by network plugin cni"
Jun 20 21:11:23 minikube cri-dockerd[1368]: time="2025-06-20T21:11:23Z" level=info msg="Setting cgroupDriver systemd"
Jun 20 21:11:23 minikube cri-dockerd[1368]: time="2025-06-20T21:11:23Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jun 20 21:11:23 minikube cri-dockerd[1368]: time="2025-06-20T21:11:23Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jun 20 21:11:23 minikube cri-dockerd[1368]: time="2025-06-20T21:11:23Z" level=info msg="Start cri-dockerd grpc backend"
Jun 20 21:11:23 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jun 20 21:11:24 minikube cri-dockerd[1368]: time="2025-06-20T21:11:24Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-ftzhx_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"1b75e38b41d4d979d0b354c8c7e3076f0c899b23672f828e4ab9258f08454311\""
Jun 20 21:11:24 minikube cri-dockerd[1368]: time="2025-06-20T21:11:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2594c80110763d47d4d2ef8c9fd2b8442ba5e72c0dfb92411ea0b4f2a637d43e/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jun 20 21:11:24 minikube cri-dockerd[1368]: time="2025-06-20T21:11:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/85fb7d31fc506573f2b0d402568f3825682d1e44fa496f0d79253eac8ba73785/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jun 20 21:11:24 minikube cri-dockerd[1368]: time="2025-06-20T21:11:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/77e60866ff5049aad44ec3f5c8d019cbcbaaf98549a8ceebffd5c1cc344b1da4/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jun 20 21:11:24 minikube cri-dockerd[1368]: time="2025-06-20T21:11:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/79a0aa2df96f30080e19ce25e8834eb42442ab897ea96b27ebccde92dbccc70d/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jun 20 21:11:27 minikube cri-dockerd[1368]: time="2025-06-20T21:11:27Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jun 20 21:11:28 minikube cri-dockerd[1368]: time="2025-06-20T21:11:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/11ee7acc2a766cd563a53d130a594a4237b0a2ffa839549d83bf6c654619b848/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Jun 20 21:11:28 minikube cri-dockerd[1368]: time="2025-06-20T21:11:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ded657fde100a4d14b4f37818c15dcf04ec370c338a5f86601002226469c72f9/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Jun 20 21:11:28 minikube cri-dockerd[1368]: time="2025-06-20T21:11:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8d840714cea63524e0927166f5830a6c426a312047c404b14ae69b2e85a91546/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Jun 20 21:11:58 minikube dockerd[1050]: time="2025-06-20T21:11:58.864356374Z" level=info msg="ignoring event" container=4fc011faf4f9b95841185b71870e13bbf374745234822fcd539b9250f1da1de9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 20 21:25:54 minikube cri-dockerd[1368]: time="2025-06-20T21:25:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4122a9841bd2359e143de81b0dfcb0962e2d15e95c79ea19ead6d2427e3b41d1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 20 21:25:56 minikube dockerd[1050]: time="2025-06-20T21:25:56.400333994Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 20 21:25:56 minikube dockerd[1050]: time="2025-06-20T21:25:56.400490269Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 20 21:26:14 minikube dockerd[1050]: time="2025-06-20T21:26:14.419557015Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 20 21:26:14 minikube dockerd[1050]: time="2025-06-20T21:26:14.419671069Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 20 21:26:44 minikube dockerd[1050]: time="2025-06-20T21:26:44.422892091Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 20 21:26:44 minikube dockerd[1050]: time="2025-06-20T21:26:44.423032975Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
892b5b52e718f       6e38f40d628db       15 minutes ago      Running             storage-provisioner       2                   ded657fde100a       storage-provisioner
d8a2a3d768310       1cf5f116067c6       15 minutes ago      Running             coredns                   1                   8d840714cea63       coredns-674b8bbfcf-ftzhx
4fc011faf4f9b       6e38f40d628db       15 minutes ago      Exited              storage-provisioner       1                   ded657fde100a       storage-provisioner
473d2cb1204ae       b79c189b052cd       15 minutes ago      Running             kube-proxy                1                   11ee7acc2a766       kube-proxy-h5rgz
87c6c37437b13       c6ab243b29f82       16 minutes ago      Running             kube-apiserver            1                   79a0aa2df96f3       kube-apiserver-minikube
d4f8d2d08da21       ef43894fa110c       16 minutes ago      Running             kube-controller-manager   1                   77e60866ff504       kube-controller-manager-minikube
b2d0fc6862df0       398c985c0d950       16 minutes ago      Running             kube-scheduler            1                   85fb7d31fc506       kube-scheduler-minikube
0b937f3295c6b       499038711c081       16 minutes ago      Running             etcd                      1                   2594c80110763       etcd-minikube
e7427f5fd8e92       1cf5f116067c6       2 days ago          Exited              coredns                   0                   1b75e38b41d4d       coredns-674b8bbfcf-ftzhx
6acafc683e7fe       b79c189b052cd       2 days ago          Exited              kube-proxy                0                   53da4c1420ca2       kube-proxy-h5rgz
e93f1976f3edb       ef43894fa110c       2 days ago          Exited              kube-controller-manager   0                   2a371ca37aefb       kube-controller-manager-minikube
6d598c7bc04ae       398c985c0d950       2 days ago          Exited              kube-scheduler            0                   0d223738da27e       kube-scheduler-minikube
16ce3bbf6d38a       c6ab243b29f82       2 days ago          Exited              kube-apiserver            0                   1177ee51249eb       kube-apiserver-minikube
b16fe919117f7       499038711c081       2 days ago          Exited              etcd                      0                   63ffc0ad70989       etcd-minikube


==> coredns [d8a2a3d76831] <==
maxprocs: Leaving GOMAXPROCS=12: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:36033 - 7027 "HINFO IN 1125146670659464839.7200764610032770373. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.045785886s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> coredns [e7427f5fd8e9] <==
maxprocs: Leaving GOMAXPROCS=12: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:50009 - 8686 "HINFO IN 4484690103895164333.4526084662488811133. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.258005946s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_06_18T13_28_02_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 18 Jun 2025 12:27:59 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 20 Jun 2025 21:27:25 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 20 Jun 2025 21:23:41 +0000   Wed, 18 Jun 2025 12:27:58 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 20 Jun 2025 21:23:41 +0000   Wed, 18 Jun 2025 12:27:58 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 20 Jun 2025 21:23:41 +0000   Wed, 18 Jun 2025 12:27:58 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 20 Jun 2025 21:23:41 +0000   Wed, 18 Jun 2025 12:28:00 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  153703184Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16053176Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  153703184Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16053176Ki
  pods:               110
System Info:
  Machine ID:                 52b1ea6fca3d4337a61f2c9fdc4fd81d
  System UUID:                b75c6a0b-f7ef-4752-a7c1-7a15f4d509bf
  Boot ID:                    811b6a0d-93c1-4cec-b76a-3c35fe777d79
  Kernel Version:             6.11.0-26-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     flask-tasks-app-8687db88b9-qgdgq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         94s
  kube-system                 coredns-674b8bbfcf-ftzhx            100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     2d8h
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         2d8h
  kube-system                 kube-apiserver-minikube             250m (2%)     0 (0%)      0 (0%)           0 (0%)         2d8h
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         2d8h
  kube-system                 kube-proxy-h5rgz                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d8h
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         2d8h
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d8h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 15m                kube-proxy       
  Normal   Starting                 16m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  16m (x8 over 16m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    16m (x8 over 16m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     16m (x7 over 16m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  16m                kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 16m                kubelet          Node minikube has been rebooted, boot id: 811b6a0d-93c1-4cec-b76a-3c35fe777d79
  Normal   RegisteredNode           15m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jun20 20:44]   #1  #3
[  +0.080040] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.VMD0.PEG0], AE_NOT_FOUND (20240322/dswload2-162)
[  +0.000011] ACPI Error: AE_NOT_FOUND, During name lookup/catalog (20240322/psobject-220)
[  +0.000008] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.VMD0.RP05], AE_NOT_FOUND (20240322/dswload2-162)
[  +0.000003] ACPI Error: AE_NOT_FOUND, During name lookup/catalog (20240322/psobject-220)
[  +0.894756] pnp 00:05: disabling [mem 0xc0000000-0xcfffffff] because it overlaps 0000:00:02.0 BAR 9 [mem 0x00000000-0xdfffffff 64bit pref]
[  +0.054633] hpet_acpi_add: no address or irqs in _CRS
[  +0.015376] i8042: Warning: Keylock active
[  +0.004808] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000080] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000000] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.028266] ENERGY_PERF_BIAS: Set to 'normal', was 'performance'
[  +0.149390] wmi_bus wmi_bus-PNP0C14:02: [Firmware Bug]: WQBC data block query control method not found
[  +1.844226] resource: resource sanity check: requesting [mem 0x00000000fedc0000-0x00000000fedcffff], which spans more than pnp 00:05 [mem 0xfedc0000-0xfedc7fff]
[  +0.000006] caller igen6_probe+0x1c9/0x610 [igen6_edac] mapping multiple BARs
[  +1.352963] ACPI Warning: \_SB.PC00.XHCI.RHUB.HS10._DSM: Argument #4 type mismatch - Found [Integer], ACPI requires [Package] (20240322/nsarguments-61)
[  +0.230053] iwlwifi 0000:00:14.3: reporting RF_KILL (radio disabled)
[  +0.000013] iwlwifi 0000:00:14.3: RF_KILL bit toggled to disable radio.
[  +0.000739] iwlwifi 0000:00:14.3: HCMD_ACTIVE already clear for command NVM_ACCESS_COMPLETE
[  +0.533273] kauditd_printk_skb: 153 callbacks suppressed
[  +0.106146] block nvme0n1: No UUID available providing old NGUID
[Jun20 20:45] kauditd_printk_skb: 1 callbacks suppressed
[ +16.754668] kauditd_printk_skb: 102 callbacks suppressed
[  +7.453871] iwlwifi 0000:00:14.3: RF_KILL bit toggled to enable radio.
[  +0.000010] iwlwifi 0000:00:14.3: reporting RF_KILL (radio enabled)
[Jun20 20:46] warning: `ThreadPoolForeg' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211
[Jun20 20:48] kauditd_printk_skb: 23 callbacks suppressed
[Jun20 20:50] kauditd_printk_skb: 23 callbacks suppressed
[ +34.147437] kauditd_printk_skb: 52 callbacks suppressed
[Jun20 20:54] kauditd_printk_skb: 23 callbacks suppressed
[ +31.356573] kauditd_printk_skb: 44 callbacks suppressed
[Jun20 20:55] kauditd_printk_skb: 21 callbacks suppressed


==> etcd [0b937f3295c6] <==
{"level":"warn","ts":"2025-06-20T21:11:25.087670Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"warn","ts":"2025-06-20T21:11:25.088024Z","caller":"etcdmain/config.go:389","msg":"--proxy-refresh-interval is deprecated in 3.5 and will be decommissioned in 3.6."}
{"level":"info","ts":"2025-06-20T21:11:25.088055Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-06-20T21:11:25.088206Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-06-20T21:11:25.088251Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-06-20T21:11:25.088273Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-06-20T21:11:25.088327Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-20T21:11:25.091775Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-06-20T21:11:25.092488Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-06-20T21:11:25.107839Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"14.564182ms"}
{"level":"info","ts":"2025-06-20T21:11:25.157106Z","caller":"etcdserver/server.go:513","msg":"recovered v2 store from snapshot","snapshot-index":10001,"snapshot-size":"7.1 kB"}
{"level":"info","ts":"2025-06-20T21:11:25.157226Z","caller":"etcdserver/server.go:526","msg":"recovered v3 backend from snapshot","backend-size-bytes":1339392,"backend-size":"1.3 MB","backend-size-in-use-bytes":602112,"backend-size-in-use":"602 kB"}
{"level":"info","ts":"2025-06-20T21:11:25.214790Z","caller":"etcdserver/raft.go:541","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":14268}
{"level":"info","ts":"2025-06-20T21:11:25.215173Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-06-20T21:11:25.215396Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2025-06-20T21:11:25.215512Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 2, commit: 14268, applied: 10001, lastindex: 14268, lastterm: 2]"}
{"level":"info","ts":"2025-06-20T21:11:25.215929Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-20T21:11:25.216015Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-06-20T21:11:25.216046Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2025-06-20T21:11:25.217028Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-06-20T21:11:25.217816Z","caller":"mvcc/kvstore.go:348","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":11182}
{"level":"info","ts":"2025-06-20T21:11:25.221874Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":11447}
{"level":"info","ts":"2025-06-20T21:11:25.222002Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":14268}
{"level":"info","ts":"2025-06-20T21:11:25.222928Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-06-20T21:11:25.224876Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-06-20T21:11:25.225385Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-06-20T21:11:25.225500Z","caller":"etcdserver/server.go:866","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.21","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-20T21:11:25.225771Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-06-20T21:11:25.226082Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-20T21:11:25.226229Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-20T21:11:25.226258Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-20T21:11:25.227103Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-20T21:11:25.230520Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-20T21:11:25.230848Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-20T21:11:25.230930Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-20T21:11:25.231204Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-06-20T21:11:25.231297Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-06-20T21:11:25.917370Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-06-20T21:11:25.917419Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-06-20T21:11:25.917438Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-06-20T21:11:25.917452Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-06-20T21:11:25.917502Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-06-20T21:11:25.917572Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-06-20T21:11:25.917590Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-06-20T21:11:25.919102Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-06-20T21:11:25.919121Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-20T21:11:25.919167Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-20T21:11:25.919310Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-06-20T21:11:25.919406Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-06-20T21:11:25.920394Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-20T21:11:25.920401Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-20T21:11:25.921163Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-06-20T21:11:25.921163Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-06-20T21:21:26.102318Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11760}
{"level":"info","ts":"2025-06-20T21:21:26.106109Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":11760,"took":"3.428442ms","hash":303678025,"current-db-size-bytes":1859584,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1859584,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-06-20T21:21:26.106153Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":303678025,"revision":11760,"compact-revision":11182}
{"level":"info","ts":"2025-06-20T21:26:26.110137Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11999}
{"level":"info","ts":"2025-06-20T21:26:26.112666Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":11999,"took":"2.146584ms","hash":3164014257,"current-db-size-bytes":1859584,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1466368,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-06-20T21:26:26.112723Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3164014257,"revision":11999,"compact-revision":11760}


==> etcd [b16fe919117f] <==
{"level":"info","ts":"2025-06-18T15:02:59.104580Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7583,"took":"1.396933ms","hash":2714450838,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":823296,"current-db-size-in-use":"823 kB"}
{"level":"info","ts":"2025-06-18T15:02:59.104630Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2714450838,"revision":7583,"compact-revision":7343}
{"level":"info","ts":"2025-06-18T15:07:36.809760Z","caller":"etcdserver/server.go:1476","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-06-18T15:07:36.813090Z","caller":"etcdserver/server.go:2539","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2025-06-18T15:07:36.813585Z","caller":"etcdserver/server.go:2569","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2025-06-18T15:07:59.109874Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7823}
{"level":"info","ts":"2025-06-18T15:07:59.111940Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7823,"took":"1.605884ms","hash":843138008,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":823296,"current-db-size-in-use":"823 kB"}
{"level":"info","ts":"2025-06-18T15:07:59.111993Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":843138008,"revision":7823,"compact-revision":7583}
{"level":"info","ts":"2025-06-18T15:12:59.117524Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8063}
{"level":"info","ts":"2025-06-18T15:12:59.121382Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8063,"took":"2.842515ms","hash":2171933826,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":815104,"current-db-size-in-use":"815 kB"}
{"level":"info","ts":"2025-06-18T15:12:59.121461Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2171933826,"revision":8063,"compact-revision":7823}
{"level":"info","ts":"2025-06-18T15:17:59.125076Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8302}
{"level":"info","ts":"2025-06-18T15:17:59.127188Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8302,"took":"1.736081ms","hash":3309019815,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":819200,"current-db-size-in-use":"819 kB"}
{"level":"info","ts":"2025-06-18T15:17:59.127239Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3309019815,"revision":8302,"compact-revision":8063}
{"level":"info","ts":"2025-06-18T15:22:59.132624Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8543}
{"level":"info","ts":"2025-06-18T15:22:59.134637Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8543,"took":"1.640683ms","hash":126567694,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":815104,"current-db-size-in-use":"815 kB"}
{"level":"info","ts":"2025-06-18T15:22:59.134692Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":126567694,"revision":8543,"compact-revision":8302}
{"level":"info","ts":"2025-06-18T15:27:59.140938Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8782}
{"level":"info","ts":"2025-06-18T15:27:59.142920Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8782,"took":"1.556182ms","hash":2789521493,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":811008,"current-db-size-in-use":"811 kB"}
{"level":"info","ts":"2025-06-18T15:27:59.142972Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2789521493,"revision":8782,"compact-revision":8543}
{"level":"info","ts":"2025-06-18T15:32:59.146600Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9023}
{"level":"info","ts":"2025-06-18T15:32:59.148359Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":9023,"took":"1.42529ms","hash":1158575707,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":819200,"current-db-size-in-use":"819 kB"}
{"level":"info","ts":"2025-06-18T15:32:59.148413Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1158575707,"revision":9023,"compact-revision":8782}
{"level":"info","ts":"2025-06-18T15:37:59.154709Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9263}
{"level":"info","ts":"2025-06-18T15:37:59.156891Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":9263,"took":"1.806416ms","hash":4285321425,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":815104,"current-db-size-in-use":"815 kB"}
{"level":"info","ts":"2025-06-18T15:37:59.156952Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4285321425,"revision":9263,"compact-revision":9023}
{"level":"info","ts":"2025-06-18T15:42:59.161695Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9502}
{"level":"info","ts":"2025-06-18T15:42:59.163739Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":9502,"took":"1.654523ms","hash":1336050488,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":811008,"current-db-size-in-use":"811 kB"}
{"level":"info","ts":"2025-06-18T15:42:59.163791Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1336050488,"revision":9502,"compact-revision":9263}
{"level":"info","ts":"2025-06-18T15:47:59.166130Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9743}
{"level":"info","ts":"2025-06-18T15:47:59.167183Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":9743,"took":"824.063¬µs","hash":809203896,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":811008,"current-db-size-in-use":"811 kB"}
{"level":"info","ts":"2025-06-18T15:47:59.167213Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":809203896,"revision":9743,"compact-revision":9502}
{"level":"info","ts":"2025-06-18T15:52:59.172748Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9982}
{"level":"info","ts":"2025-06-18T15:52:59.174832Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":9982,"took":"1.670663ms","hash":622830008,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":815104,"current-db-size-in-use":"815 kB"}
{"level":"info","ts":"2025-06-18T15:52:59.174887Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":622830008,"revision":9982,"compact-revision":9743}
{"level":"info","ts":"2025-06-18T15:57:59.179019Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10222}
{"level":"info","ts":"2025-06-18T15:57:59.180844Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":10222,"took":"1.465569ms","hash":2893534980,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":815104,"current-db-size-in-use":"815 kB"}
{"level":"info","ts":"2025-06-18T15:57:59.180897Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2893534980,"revision":10222,"compact-revision":9982}
{"level":"info","ts":"2025-06-18T16:02:59.186767Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10463}
{"level":"info","ts":"2025-06-18T16:02:59.188838Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":10463,"took":"1.665276ms","hash":687117400,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":811008,"current-db-size-in-use":"811 kB"}
{"level":"info","ts":"2025-06-18T16:02:59.188900Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":687117400,"revision":10463,"compact-revision":10222}
{"level":"info","ts":"2025-06-18T16:07:59.194029Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10702}
{"level":"info","ts":"2025-06-18T16:07:59.195818Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":10702,"took":"1.417005ms","hash":3929199810,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":811008,"current-db-size-in-use":"811 kB"}
{"level":"info","ts":"2025-06-18T16:07:59.195871Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3929199810,"revision":10702,"compact-revision":10463}
{"level":"info","ts":"2025-06-18T16:12:59.202178Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10942}
{"level":"info","ts":"2025-06-18T16:12:59.204281Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":10942,"took":"1.684791ms","hash":522046451,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":823296,"current-db-size-in-use":"823 kB"}
{"level":"info","ts":"2025-06-18T16:12:59.204354Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":522046451,"revision":10942,"compact-revision":10702}
{"level":"info","ts":"2025-06-18T16:17:59.208268Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11182}
{"level":"info","ts":"2025-06-18T16:17:59.209723Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":11182,"took":"1.159557ms","hash":3701690904,"current-db-size-bytes":1339392,"current-db-size":"1.3 MB","current-db-size-in-use-bytes":827392,"current-db-size-in-use":"827 kB"}
{"level":"info","ts":"2025-06-18T16:17:59.209763Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3701690904,"revision":11182,"compact-revision":10942}
{"level":"info","ts":"2025-06-18T16:18:30.200252Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-06-18T16:18:30.200348Z","caller":"embed/etcd.go:408","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-06-18T16:18:37.201786Z","caller":"etcdserver/server.go:1546","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"warn","ts":"2025-06-18T16:18:37.202035Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-18T16:18:37.202070Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-18T16:18:37.202149Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-18T16:18:37.202161Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-06-18T16:18:37.205776Z","caller":"embed/etcd.go:613","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-18T16:18:37.205928Z","caller":"embed/etcd.go:618","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-18T16:18:37.205984Z","caller":"embed/etcd.go:410","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 21:27:27 up 42 min,  0 users,  load average: 0.63, 0.83, 0.74
Linux minikube 6.11.0-26-generic #26~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Apr 17 19:20:47 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [16ce3bbf6d38] <==
W0618 16:18:35.809523       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:35.821814       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:35.837239       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:35.843195       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:35.844856       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:35.867485       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:35.893638       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:35.897701       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:35.926618       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:35.942872       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:35.991320       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:36.011403       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:36.076781       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:38.217876       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:38.318138       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:38.429525       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:38.534388       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:38.616059       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:38.752114       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:38.786427       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:38.816597       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:38.837540       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:38.938175       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.076217       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.085545       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.107217       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.149199       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.149484       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.155395       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.160780       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.165566       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.206595       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.220688       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.243623       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.248547       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.276724       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.320151       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.336442       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.390409       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.402873       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.407251       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.417205       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.492652       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.543006       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.551201       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.573997       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.580221       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.597248       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.704946       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.798200       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.815630       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.901144       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.939263       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.953657       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:39.987777       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:40.008279       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:40.037514       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:40.163947       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:40.170496       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 16:18:40.172908       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [87c6c37437b1] <==
I0620 21:11:26.962700       1 aggregator.go:169] waiting for initial CRD sync...
I0620 21:11:26.962716       1 controller.go:119] Starting legacy_token_tracking_controller
I0620 21:11:26.962736       1 shared_informer.go:350] "Waiting for caches to sync" controller="configmaps"
I0620 21:11:26.962758       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0620 21:11:26.962778       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I0620 21:11:26.962805       1 shared_informer.go:350] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I0620 21:11:26.962833       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0620 21:11:26.962839       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I0620 21:11:26.962863       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0620 21:11:26.962872       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0620 21:11:26.962933       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0620 21:11:26.962967       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0620 21:11:26.962873       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0620 21:11:26.963245       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0620 21:11:26.962947       1 controller.go:142] Starting OpenAPI controller
I0620 21:11:26.963527       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0620 21:11:26.963005       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0620 21:11:26.963042       1 controller.go:90] Starting OpenAPI V3 controller
I0620 21:11:26.963053       1 naming_controller.go:299] Starting NamingConditionController
I0620 21:11:26.963062       1 establishing_controller.go:81] Starting EstablishingController
I0620 21:11:26.963073       1 local_available_controller.go:156] Starting LocalAvailability controller
I0620 21:11:26.963874       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0620 21:11:26.963206       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0620 21:11:26.963226       1 crd_finalizer.go:269] Starting CRDFinalizer
I0620 21:11:26.964240       1 repairip.go:200] Starting ipallocator-repair-controller
I0620 21:11:26.964256       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0620 21:11:27.029680       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0620 21:11:27.063408       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0620 21:11:27.063447       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0620 21:11:27.063417       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0620 21:11:27.063586       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0620 21:11:27.063613       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0620 21:11:27.063629       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0620 21:11:27.063770       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0620 21:11:27.063771       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0620 21:11:27.063815       1 aggregator.go:171] initial CRD sync complete...
I0620 21:11:27.063828       1 autoregister_controller.go:144] Starting autoregister controller
I0620 21:11:27.063834       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0620 21:11:27.063842       1 cache.go:39] Caches are synced for autoregister controller
I0620 21:11:27.063852       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0620 21:11:27.063962       1 cache.go:39] Caches are synced for LocalAvailability controller
I0620 21:11:27.064305       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0620 21:11:27.066132       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
E0620 21:11:27.069611       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0620 21:11:27.070686       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0620 21:11:27.074520       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0620 21:11:27.074548       1 policy_source.go:240] refreshing policies
I0620 21:11:27.090798       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0620 21:11:27.380965       1 controller.go:667] quota admission added evaluator for: endpoints
I0620 21:11:27.968578       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0620 21:11:28.201978       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0620 21:11:28.201978       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0620 21:11:30.445106       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0620 21:11:30.702285       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0620 21:11:30.753565       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0620 21:11:30.998078       1 controller.go:667] quota admission added evaluator for: deployments.apps
E0620 21:11:37.493083       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: dfe3546c-24ff-4743-870d-0c5840060af5, UID in object meta: "
I0620 21:21:26.992670       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0620 21:26:03.192165       1 alloc.go:328] "allocated clusterIPs" service="default/flask-tasks-service" clusterIPs={"IPv4":"10.100.42.209"}
I0620 21:26:03.193023       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [d4f8d2d08da2] <==
I0620 21:11:30.247281       1 namespace_controller.go:202] "Starting namespace controller" logger="namespace-controller"
I0620 21:11:30.247294       1 shared_informer.go:350] "Waiting for caches to sync" controller="namespace"
I0620 21:11:30.287662       1 controllermanager.go:778] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0620 21:11:30.287690       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="device-taint-eviction-controller" requiredFeatureGates=["DynamicResourceAllocation","DRADeviceTaints"]
I0620 21:11:30.287714       1 controllermanager.go:756] "Warning: skipping controller" controller="storage-version-migrator-controller"
I0620 21:11:30.287768       1 pvc_protection_controller.go:168] "Starting PVC protection controller" logger="persistentvolumeclaim-protection-controller"
I0620 21:11:30.287789       1 shared_informer.go:350] "Waiting for caches to sync" controller="PVC protection"
I0620 21:11:30.306713       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0620 21:11:30.309199       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0620 21:11:30.321982       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0620 21:11:30.324856       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0620 21:11:30.324975       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0620 21:11:30.330713       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0620 21:11:30.338309       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0620 21:11:30.341850       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0620 21:11:30.342810       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0620 21:11:30.342909       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0620 21:11:30.343106       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0620 21:11:30.348339       1 shared_informer.go:357] "Caches are synced" controller="node"
I0620 21:11:30.348393       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0620 21:11:30.348421       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0620 21:11:30.348470       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0620 21:11:30.348480       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0620 21:11:30.348489       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0620 21:11:30.348661       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0620 21:11:30.351740       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0620 21:11:30.351861       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0620 21:11:30.355260       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0620 21:11:30.368611       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0620 21:11:30.370974       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0620 21:11:30.373238       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0620 21:11:30.380590       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0620 21:11:30.384120       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0620 21:11:30.388713       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0620 21:11:30.388765       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0620 21:11:30.388894       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0620 21:11:30.391458       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0620 21:11:30.392671       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0620 21:11:30.394814       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0620 21:11:30.396964       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0620 21:11:30.401295       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0620 21:11:30.402524       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0620 21:11:30.403789       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0620 21:11:30.405062       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0620 21:11:30.410699       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0620 21:11:30.545992       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0620 21:11:30.565316       1 shared_informer.go:357] "Caches are synced" controller="job"
I0620 21:11:30.576477       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0620 21:11:30.590584       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0620 21:11:30.590733       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0620 21:11:30.590827       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0620 21:11:30.590889       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0620 21:11:30.607640       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0620 21:11:30.634626       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0620 21:11:30.663265       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0620 21:11:30.716015       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0620 21:11:31.125841       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0620 21:11:31.161701       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0620 21:11:31.161748       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0620 21:11:31.161763       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-controller-manager [e93f1976f3ed] <==
I0618 12:28:06.054989       1 controller.go:173] "Starting ephemeral volume controller" logger="ephemeral-volume-controller"
I0618 12:28:06.055001       1 shared_informer.go:350] "Waiting for caches to sync" controller="ephemeral"
I0618 12:28:06.204913       1 controllermanager.go:778] "Started controller" controller="legacy-serviceaccount-token-cleaner-controller"
I0618 12:28:06.204990       1 legacy_serviceaccount_token_cleaner.go:103] "Starting legacy service account token cleaner controller" logger="legacy-serviceaccount-token-cleaner-controller"
I0618 12:28:06.205009       1 shared_informer.go:350] "Waiting for caches to sync" controller="legacy-service-account-token-cleaner"
I0618 12:28:06.211682       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0618 12:28:06.217055       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0618 12:28:06.221494       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0618 12:28:06.225018       1 shared_informer.go:357] "Caches are synced" controller="job"
I0618 12:28:06.230377       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0618 12:28:06.236484       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0618 12:28:06.241839       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0618 12:28:06.248249       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0618 12:28:06.253107       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0618 12:28:06.255212       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0618 12:28:06.255247       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0618 12:28:06.256461       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0618 12:28:06.256466       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0618 12:28:06.257006       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0618 12:28:06.257666       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0618 12:28:06.258912       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0618 12:28:06.302588       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0618 12:28:06.302754       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0618 12:28:06.302588       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0618 12:28:06.303040       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0618 12:28:06.303112       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0618 12:28:06.303588       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0618 12:28:06.304703       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0618 12:28:06.305825       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0618 12:28:06.305859       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0618 12:28:06.305965       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0618 12:28:06.305978       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0618 12:28:06.306875       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0618 12:28:06.306892       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0618 12:28:06.308433       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0618 12:28:06.308543       1 shared_informer.go:357] "Caches are synced" controller="node"
I0618 12:28:06.308602       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0618 12:28:06.308628       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0618 12:28:06.308637       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0618 12:28:06.308645       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0618 12:28:06.309640       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0618 12:28:06.317534       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0618 12:28:06.324010       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0618 12:28:06.358021       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0618 12:28:06.362630       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0618 12:28:06.403202       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0618 12:28:06.403202       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0618 12:28:06.506182       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0618 12:28:06.506330       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0618 12:28:06.555770       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0618 12:28:06.555804       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0618 12:28:06.555816       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0618 12:28:06.557047       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0618 12:28:06.611298       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0618 12:28:06.612388       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0618 12:28:07.022297       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0618 12:28:07.055789       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0618 12:28:07.055807       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0618 12:28:07.055813       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0618 14:28:04.956187       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-hlg5x" approvedExpiration="1h0m0s"


==> kube-proxy [473d2cb1204a] <==
I0620 21:11:28.941590       1 server_linux.go:63] "Using iptables proxy"
I0620 21:11:29.089158       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0620 21:11:29.089306       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0620 21:11:29.124307       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0620 21:11:29.124416       1 server_linux.go:145] "Using iptables Proxier"
I0620 21:11:29.131456       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0620 21:11:29.137231       1 server.go:516] "Version info" version="v1.33.1"
I0620 21:11:29.137264       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0620 21:11:29.139676       1 config.go:199] "Starting service config controller"
I0620 21:11:29.139762       1 config.go:105] "Starting endpoint slice config controller"
I0620 21:11:29.139941       1 config.go:440] "Starting serviceCIDR config controller"
I0620 21:11:29.140139       1 config.go:329] "Starting node config controller"
I0620 21:11:29.140372       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0620 21:11:29.140414       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0620 21:11:29.140416       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0620 21:11:29.140518       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0620 21:11:29.241394       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0620 21:11:29.241428       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0620 21:11:29.241457       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0620 21:11:29.241403       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"


==> kube-proxy [6acafc683e7f] <==
I0618 12:28:07.917879       1 server_linux.go:63] "Using iptables proxy"
I0618 12:28:08.003000       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0618 12:28:08.003048       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0618 12:28:08.037372       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0618 12:28:08.037453       1 server_linux.go:145] "Using iptables Proxier"
I0618 12:28:08.047961       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0618 12:28:08.054490       1 server.go:516] "Version info" version="v1.33.1"
I0618 12:28:08.054517       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0618 12:28:08.055778       1 config.go:199] "Starting service config controller"
I0618 12:28:08.055803       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0618 12:28:08.056678       1 config.go:329] "Starting node config controller"
I0618 12:28:08.056701       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0618 12:28:08.057033       1 config.go:105] "Starting endpoint slice config controller"
I0618 12:28:08.057049       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0618 12:28:08.057045       1 config.go:440] "Starting serviceCIDR config controller"
I0618 12:28:08.057065       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0618 12:28:08.155997       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0618 12:28:08.157119       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0618 12:28:08.157132       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0618 12:28:08.159936       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [6d598c7bc04a] <==
I0618 12:27:58.520826       1 serving.go:386] Generated self-signed cert in-memory
W0618 12:27:59.596523       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0618 12:27:59.596558       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0618 12:27:59.596567       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0618 12:27:59.596576       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0618 12:27:59.608183       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0618 12:27:59.608216       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0618 12:27:59.610203       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0618 12:27:59.610238       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0618 12:27:59.610512       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0618 12:27:59.610572       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0618 12:27:59.612547       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0618 12:27:59.612742       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0618 12:27:59.612549       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0618 12:27:59.612795       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0618 12:27:59.612812       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0618 12:27:59.612853       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0618 12:27:59.612847       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0618 12:27:59.612879       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0618 12:27:59.612927       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0618 12:27:59.612958       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0618 12:27:59.613051       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0618 12:27:59.613074       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0618 12:27:59.613087       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0618 12:27:59.613099       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0618 12:27:59.613110       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0618 12:27:59.613147       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0618 12:28:00.420468       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0618 12:28:00.430952       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0618 12:28:00.446497       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0618 12:28:00.506117       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0618 12:28:00.512599       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0618 12:28:00.547530       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0618 12:28:00.747356       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0618 12:28:00.791340       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0618 12:28:00.800484       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0618 12:28:00.814715       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
I0618 12:28:03.810783       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0618 16:18:30.211185       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
I0618 16:18:30.211198       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0618 16:18:30.211254       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0618 16:18:30.211290       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [b2d0fc6862df] <==
I0620 21:11:26.379523       1 serving.go:386] Generated self-signed cert in-memory
W0620 21:11:26.987249       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0620 21:11:26.987303       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0620 21:11:26.987318       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0620 21:11:26.987327       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0620 21:11:27.005141       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0620 21:11:27.005174       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0620 21:11:27.009130       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0620 21:11:27.009808       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0620 21:11:27.010078       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0620 21:11:27.010679       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0620 21:11:27.110003       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Jun 20 21:11:25 minikube kubelet[1606]: E0620 21:11:25.275398    1606 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Jun 20 21:11:25 minikube kubelet[1606]: E0620 21:11:25.278512    1606 reflector.go:200] "Failed to watch" err="failed to list *v1.RuntimeClass: Get \"https://192.168.49.2:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
Jun 20 21:11:25 minikube kubelet[1606]: E0620 21:11:25.286685    1606 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Jun 20 21:11:25 minikube kubelet[1606]: E0620 21:11:25.295684    1606 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Jun 20 21:11:25 minikube kubelet[1606]: I0620 21:11:25.680690    1606 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Jun 20 21:11:26 minikube kubelet[1606]: E0620 21:11:26.301659    1606 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Jun 20 21:11:26 minikube kubelet[1606]: E0620 21:11:26.301784    1606 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Jun 20 21:11:26 minikube kubelet[1606]: E0620 21:11:26.301892    1606 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Jun 20 21:11:26 minikube kubelet[1606]: E0620 21:11:26.302012    1606 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Jun 20 21:11:27 minikube kubelet[1606]: I0620 21:11:27.088688    1606 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Jun 20 21:11:27 minikube kubelet[1606]: I0620 21:11:27.088830    1606 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.088861    1606 kubelet_node_status.go:548] "Error updating node status, will retry" err="error getting node \"minikube\": node \"minikube\" not found"
Jun 20 21:11:27 minikube kubelet[1606]: I0620 21:11:27.091345    1606 kuberuntime_manager.go:1746] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jun 20 21:11:27 minikube kubelet[1606]: I0620 21:11:27.092140    1606 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.102805    1606 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.203402    1606 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.304062    1606 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.307973    1606 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.308056    1606 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.308165    1606 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.308330    1606 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.404429    1606 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Jun 20 21:11:27 minikube kubelet[1606]: I0620 21:11:27.499328    1606 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.510937    1606 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Jun 20 21:11:27 minikube kubelet[1606]: I0620 21:11:27.510975    1606 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.519652    1606 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jun 20 21:11:27 minikube kubelet[1606]: I0620 21:11:27.519729    1606 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.535176    1606 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jun 20 21:11:27 minikube kubelet[1606]: I0620 21:11:27.535258    1606 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jun 20 21:11:27 minikube kubelet[1606]: E0620 21:11:27.543346    1606 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jun 20 21:11:28 minikube kubelet[1606]: I0620 21:11:28.090775    1606 apiserver.go:52] "Watching apiserver"
Jun 20 21:11:28 minikube kubelet[1606]: I0620 21:11:28.101698    1606 desired_state_of_world_populator.go:158] "Finished populating initial desired state of world"
Jun 20 21:11:28 minikube kubelet[1606]: I0620 21:11:28.198409    1606 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/f12fbad4-973f-4ede-aa7c-28c33c98c0d3-tmp\") pod \"storage-provisioner\" (UID: \"f12fbad4-973f-4ede-aa7c-28c33c98c0d3\") " pod="kube-system/storage-provisioner"
Jun 20 21:11:28 minikube kubelet[1606]: I0620 21:11:28.198456    1606 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/fdf9af22-0128-45cc-9c82-cdad9b998c07-lib-modules\") pod \"kube-proxy-h5rgz\" (UID: \"fdf9af22-0128-45cc-9c82-cdad9b998c07\") " pod="kube-system/kube-proxy-h5rgz"
Jun 20 21:11:28 minikube kubelet[1606]: I0620 21:11:28.198482    1606 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/fdf9af22-0128-45cc-9c82-cdad9b998c07-xtables-lock\") pod \"kube-proxy-h5rgz\" (UID: \"fdf9af22-0128-45cc-9c82-cdad9b998c07\") " pod="kube-system/kube-proxy-h5rgz"
Jun 20 21:11:28 minikube kubelet[1606]: I0620 21:11:28.316715    1606 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jun 20 21:11:28 minikube kubelet[1606]: E0620 21:11:28.323550    1606 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jun 20 21:11:36 minikube kubelet[1606]: I0620 21:11:36.414410    1606 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jun 20 21:11:59 minikube kubelet[1606]: I0620 21:11:59.702615    1606 scope.go:117] "RemoveContainer" containerID="68813667f4bc021e260b20a50b8c74bf25e5055009e6a05b3679305357f9cd33"
Jun 20 21:11:59 minikube kubelet[1606]: I0620 21:11:59.703176    1606 scope.go:117] "RemoveContainer" containerID="4fc011faf4f9b95841185b71870e13bbf374745234822fcd539b9250f1da1de9"
Jun 20 21:11:59 minikube kubelet[1606]: E0620 21:11:59.703591    1606 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f12fbad4-973f-4ede-aa7c-28c33c98c0d3)\"" pod="kube-system/storage-provisioner" podUID="f12fbad4-973f-4ede-aa7c-28c33c98c0d3"
Jun 20 21:12:10 minikube kubelet[1606]: I0620 21:12:10.147314    1606 scope.go:117] "RemoveContainer" containerID="4fc011faf4f9b95841185b71870e13bbf374745234822fcd539b9250f1da1de9"
Jun 20 21:25:53 minikube kubelet[1606]: I0620 21:25:53.830978    1606 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dp55c\" (UniqueName: \"kubernetes.io/projected/df16c440-17f0-41d6-a122-ab9f992da1b5-kube-api-access-dp55c\") pod \"flask-tasks-app-8687db88b9-qgdgq\" (UID: \"df16c440-17f0-41d6-a122-ab9f992da1b5\") " pod="default/flask-tasks-app-8687db88b9-qgdgq"
Jun 20 21:25:56 minikube kubelet[1606]: E0620 21:25:56.405064    1606 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rahma/flask-tasks-app:latest"
Jun 20 21:25:56 minikube kubelet[1606]: E0620 21:25:56.405180    1606 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rahma/flask-tasks-app:latest"
Jun 20 21:25:56 minikube kubelet[1606]: E0620 21:25:56.405835    1606 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:flask-tasks-app,Image:rahma/flask-tasks-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp55c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-tasks-app-8687db88b9-qgdgq_default(df16c440-17f0-41d6-a122-ab9f992da1b5): ErrImagePull: Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jun 20 21:25:56 minikube kubelet[1606]: E0620 21:25:56.407177    1606 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-tasks-app\" with ErrImagePull: \"Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-tasks-app-8687db88b9-qgdgq" podUID="df16c440-17f0-41d6-a122-ab9f992da1b5"
Jun 20 21:25:56 minikube kubelet[1606]: E0620 21:25:56.908099    1606 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-tasks-app\" with ImagePullBackOff: \"Back-off pulling image \\\"rahma/flask-tasks-app\\\": ErrImagePull: Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-tasks-app-8687db88b9-qgdgq" podUID="df16c440-17f0-41d6-a122-ab9f992da1b5"
Jun 20 21:26:14 minikube kubelet[1606]: E0620 21:26:14.423126    1606 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rahma/flask-tasks-app:latest"
Jun 20 21:26:14 minikube kubelet[1606]: E0620 21:26:14.423359    1606 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rahma/flask-tasks-app:latest"
Jun 20 21:26:14 minikube kubelet[1606]: E0620 21:26:14.423591    1606 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:flask-tasks-app,Image:rahma/flask-tasks-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp55c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-tasks-app-8687db88b9-qgdgq_default(df16c440-17f0-41d6-a122-ab9f992da1b5): ErrImagePull: Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jun 20 21:26:14 minikube kubelet[1606]: E0620 21:26:14.424847    1606 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-tasks-app\" with ErrImagePull: \"Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-tasks-app-8687db88b9-qgdgq" podUID="df16c440-17f0-41d6-a122-ab9f992da1b5"
Jun 20 21:26:28 minikube kubelet[1606]: E0620 21:26:28.147381    1606 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-tasks-app\" with ImagePullBackOff: \"Back-off pulling image \\\"rahma/flask-tasks-app\\\": ErrImagePull: Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-tasks-app-8687db88b9-qgdgq" podUID="df16c440-17f0-41d6-a122-ab9f992da1b5"
Jun 20 21:26:44 minikube kubelet[1606]: E0620 21:26:44.427409    1606 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rahma/flask-tasks-app:latest"
Jun 20 21:26:44 minikube kubelet[1606]: E0620 21:26:44.427532    1606 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rahma/flask-tasks-app:latest"
Jun 20 21:26:44 minikube kubelet[1606]: E0620 21:26:44.427781    1606 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:flask-tasks-app,Image:rahma/flask-tasks-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp55c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-tasks-app-8687db88b9-qgdgq_default(df16c440-17f0-41d6-a122-ab9f992da1b5): ErrImagePull: Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jun 20 21:26:44 minikube kubelet[1606]: E0620 21:26:44.429088    1606 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-tasks-app\" with ErrImagePull: \"Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-tasks-app-8687db88b9-qgdgq" podUID="df16c440-17f0-41d6-a122-ab9f992da1b5"
Jun 20 21:26:57 minikube kubelet[1606]: E0620 21:26:57.147561    1606 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-tasks-app\" with ImagePullBackOff: \"Back-off pulling image \\\"rahma/flask-tasks-app\\\": ErrImagePull: Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-tasks-app-8687db88b9-qgdgq" podUID="df16c440-17f0-41d6-a122-ab9f992da1b5"
Jun 20 21:27:08 minikube kubelet[1606]: E0620 21:27:08.148075    1606 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-tasks-app\" with ImagePullBackOff: \"Back-off pulling image \\\"rahma/flask-tasks-app\\\": ErrImagePull: Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-tasks-app-8687db88b9-qgdgq" podUID="df16c440-17f0-41d6-a122-ab9f992da1b5"
Jun 20 21:27:21 minikube kubelet[1606]: E0620 21:27:21.148349    1606 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-tasks-app\" with ImagePullBackOff: \"Back-off pulling image \\\"rahma/flask-tasks-app\\\": ErrImagePull: Error response from daemon: pull access denied for rahma/flask-tasks-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-tasks-app-8687db88b9-qgdgq" podUID="df16c440-17f0-41d6-a122-ab9f992da1b5"


==> storage-provisioner [4fc011faf4f9] <==
I0620 21:11:28.827324       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0620 21:11:58.837835       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [892b5b52e718] <==
W0620 21:26:28.952928       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:28.958337       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:30.963412       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:30.970365       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:32.973750       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:32.978597       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:34.985866       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:34.994039       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:36.997690       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:37.004882       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:39.009809       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:39.016762       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:41.022416       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:41.028812       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:43.034876       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:43.041510       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:45.047441       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:45.053963       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:47.060394       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:47.067067       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:49.072709       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:49.079498       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:51.084148       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:51.090849       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:53.096377       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:53.102847       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:55.107491       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:55.114690       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:57.120322       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:57.128116       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:59.133149       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:26:59.139908       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:01.144515       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:01.152008       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:03.157728       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:03.165418       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:05.169673       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:05.175713       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:07.181544       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:07.187554       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:09.193383       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:09.200963       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:11.205943       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:11.212854       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:13.218035       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:13.224328       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:15.228686       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:15.235518       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:17.243279       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:17.250140       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:19.256878       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:19.263756       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:21.270010       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:21.275752       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:23.281463       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:23.287590       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:25.294216       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:25.301665       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:27.305713       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0620 21:27:27.310172       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

